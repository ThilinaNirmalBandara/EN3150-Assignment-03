{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59027a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load data\n",
    "# ---------------------------\n",
    "image_array = np.load(\"images.npy\", allow_pickle=True)\n",
    "labels = np.load(\"labels.npy\", allow_pickle=True)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(labels)\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {encoder.classes_}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Train/Val/Test split\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(y)), test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_idx, test_size=0.15, random_state=42, stratify=y[train_idx]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_idx)}, Val size: {len(val_idx)}, Test size: {len(test_idx)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Dataset class\n",
    "# ---------------------------\n",
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, images, labels, indices, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        img = self.images[i].astype(np.uint8)\n",
    "        label = self.labels[i]\n",
    "        img = np.expand_dims(img, axis=-1)  # (H,W,1)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Enhanced Transforms\n",
    "# ---------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB for pretrained models\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = WasteDataset(image_array, y, train_idx, transform=train_transform)\n",
    "val_dataset = WasteDataset(image_array, y, val_idx, transform=val_transform)\n",
    "test_dataset = WasteDataset(image_array, y, test_idx, transform=val_transform)\n",
    "\n",
    "# Increase batch size and add workers\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=0, pin_memory=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Model Selection\n",
    "# ---------------------------\n",
    "# Option 1: Custom Improved CNN\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        self.fc1 = nn.Linear(512*4*4, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.adapt_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Option 2: Transfer Learning with ResNet50 (RECOMMENDED)\n",
    "def create_resnet_model(num_classes, pretrained=True):\n",
    "    model = models.resnet50(pretrained=pretrained)\n",
    "    # Freeze early layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last few layers for fine-tuning\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.layer3.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Replace final layer\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Option 3: EfficientNet (Alternative)\n",
    "def create_efficientnet_model(num_classes, pretrained=True):\n",
    "    model = models.efficientnet_b0(pretrained=pretrained)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last layers\n",
    "    for param in model.features[-3:].parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(model.classifier[1].in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Choose model (change here to try different models)\n",
    "USE_TRANSFER_LEARNING = True\n",
    "\n",
    "if USE_TRANSFER_LEARNING:\n",
    "    print(\"Using ResNet50 with Transfer Learning\")\n",
    "    model = create_resnet_model(num_classes, pretrained=True)\n",
    "else:\n",
    "    print(\"Using Custom Improved CNN\")\n",
    "    model = ImprovedCNN(num_classes, input_channels=3)\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Label Smoothing Loss\n",
    "# ---------------------------\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon=0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        n_class = output.size(1)\n",
    "        log_preds = F.log_softmax(output, dim=1)\n",
    "        loss = -log_preds.sum(dim=1).mean()\n",
    "        nll = F.nll_loss(log_preds, target, weight=self.weight)\n",
    "        return (1 - self.epsilon) * nll + self.epsilon * loss / n_class\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y[train_idx]),\n",
    "    y=y[train_idx]\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Use label smoothing\n",
    "criterion = LabelSmoothingCrossEntropy(epsilon=0.1, weight=class_weights)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Optimizer with different learning rates\n",
    "# ---------------------------\n",
    "if USE_TRANSFER_LEARNING:\n",
    "    # Different learning rates for different layers\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.fc.parameters(), 'lr': 0.001},\n",
    "        {'params': model.layer4.parameters(), 'lr': 0.0001},\n",
    "        {'params': model.layer3.parameters(), 'lr': 0.00005}\n",
    "    ], weight_decay=1e-4)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7\n",
    ")\n",
    "\n",
    "# Cosine annealing (alternative scheduler)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Training loop with all improvements\n",
    "# ---------------------------\n",
    "num_epochs = 50\n",
    "best_val_acc = 0\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    \n",
    "    for i, (X_batch, y_batch) in loop:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=running_loss/total, acc=correct/total)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    loop_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loop_val:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            loop_val.set_postfix(loss=val_loss/total, acc=correct/total)\n",
    "\n",
    "    val_loss = val_loss / total\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        trigger_times = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, \"best_model.pth\")\n",
    "        print(f\"✓ Best model saved with val_acc: {val_acc:.4f}\")\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Plot training history\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Test evaluation\n",
    "# ---------------------------\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop_test = tqdm(test_loader, desc=\"Testing\")\n",
    "    for X_batch, y_batch in loop_test:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# 10. Confusion matrix\n",
    "# ---------------------------\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
    "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.ylabel(\"True\", fontsize=12)\n",
    "plt.title(f\"Confusion Matrix (Test Accuracy: {test_acc*100:.2f}%)\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 11. Classification report\n",
    "# ---------------------------\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in encoder.classes_], digits=4))\n",
    "\n",
    "# ---------------------------\n",
    "# 12. Per-class accuracy\n",
    "# ---------------------------\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\"*70)\n",
    "for i, class_name in enumerate(encoder.classes_):\n",
    "    class_mask = np.array(y_true) == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = np.mean(np.array(y_pred)[class_mask] == i)\n",
    "        print(f\"{str(class_name):20s}: {class_acc*100:6.2f}% ({class_mask.sum()} samples)\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"Final test accuracy: {test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
