{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d53b05",
   "metadata": {},
   "source": [
    "# RealWaste — VGG16-BN (Using Manifest + Numpy Splits)\n",
    "\n",
    "Loads dataset from your `.npy` splits (`filepaths.npy`, `labels_encoded.npy`, `class_names.npy`,\n",
    "`split_train.npy`, `split_val.npy`, `split_test.npy`, optional `mean_std.npy`), fine-tunes\n",
    "**VGG16-BN**, and reports Accuracy + macro Precision/Recall/F1 + confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea6d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Installs (uncomment if needed)\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install scikit-learn matplotlib pandas tqdm pillow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad92820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "SPLITS_DIR = Path(\"C:/Users/User/Desktop/CNN asignment/code/pre_trained_models\")  # change if your npy files are elsewhere\n",
    "\n",
    "FILEPATHS_NPY = SPLITS_DIR / \"filepaths.npy\"\n",
    "LABELS_NPY    = SPLITS_DIR / \"labels_encoded.npy\"\n",
    "CLASSES_NPY   = SPLITS_DIR / \"class_names.npy\"\n",
    "TRAIN_NPY     = SPLITS_DIR / \"split_train.npy\"\n",
    "VAL_NPY       = SPLITS_DIR / \"split_val.npy\"\n",
    "TEST_NPY      = SPLITS_DIR / \"split_test.npy\"\n",
    "MEAN_STD_NPY  = SPLITS_DIR / \"mean_std.npy\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"./outputs/vgg16bn_realwaste_splits\").resolve()\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 133\n",
    "BATCH_SIZE = 32\n",
    "HEAD_EPOCHS = 5\n",
    "FT_EPOCHS = 20\n",
    "BASE_LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTH = 0.1\n",
    "IMG_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1db367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random, os, numpy as np, torch\n",
    "def set_seed(seed=133):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ef8568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset mean/std: [0.598393759260575, 0.6187966115872066, 0.6307503436009089] [0.1635656363186559, 0.1645716954870396, 0.18778279659653346]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " ['Cardboard', 'Food Organics', 'Glass', 'Metal', 'Miscellaneous Trash'],\n",
       " 3326,\n",
       " 713,\n",
       " 713)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load arrays\n",
    "filepaths = np.load(FILEPATHS_NPY, allow_pickle=True)\n",
    "labels    = np.load(LABELS_NPY)\n",
    "classes   = np.load(CLASSES_NPY, allow_pickle=True).tolist()\n",
    "idx_tr    = np.load(TRAIN_NPY); idx_va = np.load(VAL_NPY); idx_te = np.load(TEST_NPY)\n",
    "\n",
    "try:\n",
    "    mean_std = np.load(MEAN_STD_NPY)\n",
    "    mean = mean_std[0].tolist(); std = mean_std[1].tolist()\n",
    "    print(\"Using dataset mean/std:\", mean, std)\n",
    "except Exception:\n",
    "    mean = [0.485, 0.456, 0.406]; std = [0.229, 0.224, 0.225]\n",
    "    print(\"No mean_std.npy found; using ImageNet stats.\")\n",
    "\n",
    "len(classes), classes[:5], len(idx_tr), len(idx_va), len(idx_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34939402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3326, 713, 713)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class NpySplitDataset(Dataset):\n",
    "    def __init__(self, filepaths, labels, indices, img_size=224, train=True, mean=None, std=None):\n",
    "        self.filepaths=filepaths; self.labels=labels; self.indices=indices; self.train=train\n",
    "        self.mean = mean or [0.485,0.456,0.406]; self.std = std or [0.229,0.224,0.225]\n",
    "        if train:\n",
    "            self.tf = transforms.Compose([\n",
    "                transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
    "                transforms.RandomHorizontalFlip(p=0.5), transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "                transforms.ToTensor(), transforms.Normalize(self.mean,self.std),\n",
    "            ])\n",
    "        else:\n",
    "            self.tf = transforms.Compose([\n",
    "                transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
    "                transforms.ToTensor(), transforms.Normalize(self.mean,self.std),\n",
    "            ])\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self,i):\n",
    "        idx=int(self.indices[i]); fp=str(self.filepaths[idx]); y=int(self.labels[idx])\n",
    "        img=Image.open(fp).convert(\"RGB\"); x=self.tf(img); return x,y\n",
    "\n",
    "train_ds=NpySplitDataset(filepaths,labels,idx_tr,IMG_SIZE,True,mean,std)\n",
    "val_ds  =NpySplitDataset(filepaths,labels,idx_va,IMG_SIZE,False,mean,std)\n",
    "test_ds =NpySplitDataset(filepaths,labels,idx_te,IMG_SIZE,False,mean,std)\n",
    "\n",
    "train_dl=DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=0,pin_memory=True)\n",
    "val_dl  =DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=False,num_workers=0,pin_memory=True)\n",
    "test_dl =DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,num_workers=0,pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f26ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to C:\\Users\\User/.cache\\torch\\hub\\checkpoints\\vgg16_bn-6c64b313.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528M/528M [00:56<00:00, 9.87MB/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "m = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "m.classifier[6] = nn.Linear(m.classifier[6].in_features, len(classes))\n",
    "model = m.to(device)\n",
    "classifier_params = list(m.classifier[6].parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83666549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dl, optimizer, device, smoothing=0.1):\n",
    "    model.train(); total=correct=0; loss_sum=0.0\n",
    "    for x,y in tqdm(dl, leave=False):\n",
    "        x,y=x.to(device),y.to(device); optimizer.zero_grad()\n",
    "        out=model(x); loss=F.cross_entropy(out,y,label_smoothing=smoothing); pred=out.argmax(1)\n",
    "        loss.backward(); optimizer.step()\n",
    "        bs=y.size(0); loss_sum+=loss.item()*bs; correct+=(pred==y).sum().item(); total+=bs\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl, device):\n",
    "    model.eval(); total=correct=0; loss_sum=0.0\n",
    "    for x,y in dl:\n",
    "        x,y=x.to(device),y.to(device); out=model(x); loss=F.cross_entropy(out,y); pred=out.argmax(1)\n",
    "        bs=y.size(0); loss_sum+=loss.item()*bs; correct+=(pred==y).sum().item(); total+=bs\n",
    "    return loss_sum/total, correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3012cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[head][01/5] train_acc=0.5538  val_acc=0.7153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[head][02/5] train_acc=0.6963  val_acc=0.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[head][03/5] train_acc=0.7213  val_acc=0.7644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[head][04/5] train_acc=0.7468  val_acc=0.7686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[head][05/5] train_acc=0.7526  val_acc=0.7770\n",
      "Saved: C:\\Users\\User\\Desktop\\CNN asignment\\code\\pre_trained_models\\outputs\\vgg16bn_realwaste_splits\\vgg16bn_head_best.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Train classifier head only\n",
    "for p in model.parameters(): p.requires_grad=False\n",
    "for p in classifier_params: p.requires_grad=True\n",
    "\n",
    "opt=AdamW(filter(lambda p:p.requires_grad, model.parameters()), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "sch=CosineAnnealingLR(opt, T_max=HEAD_EPOCHS)\n",
    "\n",
    "best_val_acc=-1.0; best_state=None\n",
    "for ep in range(1, HEAD_EPOCHS+1):\n",
    "    tr_loss,tr_acc=train_one_epoch(model, train_dl, opt, device, smoothing=LABEL_SMOOTH)\n",
    "    va_loss,va_acc=evaluate(model, val_dl, device); sch.step()\n",
    "    print(f\"[head][{ep:02d}/{HEAD_EPOCHS}] train_acc={tr_acc:.4f}  val_acc={va_acc:.4f}\")\n",
    "    if va_acc>best_val_acc:\n",
    "        best_val_acc=va_acc; best_state={k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "from pathlib import Path; import torch\n",
    "if best_state is not None:\n",
    "    head_ckpt=Path(OUTPUT_DIR)/\"vgg16bn_head_best.pth\"\n",
    "    torch.save(best_state, head_ckpt); print(\"Saved:\", head_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c87553c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ft  ][01/20] train_acc=0.8070  val_acc=0.8682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ft  ][02/20] train_acc=0.9390  val_acc=0.9004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m best_val_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m; best_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, FT_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     tr_loss,tr_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_SMOOTH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     va_loss,va_acc\u001b[38;5;241m=\u001b[39mevaluate(model, val_dl, device); sch\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ft  ][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFT_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] train_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dl, optimizer, device, smoothing)\u001b[0m\n\u001b[0;32m     10\u001b[0m     out\u001b[38;5;241m=\u001b[39mmodel(x); loss\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mcross_entropy(out,y,label_smoothing\u001b[38;5;241m=\u001b[39msmoothing); pred\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(); optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 12\u001b[0m     bs\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m); loss_sum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mbs; correct\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(pred\u001b[38;5;241m==\u001b[39my)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem(); total\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mbs\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_sum\u001b[38;5;241m/\u001b[39mtotal, correct\u001b[38;5;241m/\u001b[39mtotal\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Fine-tune all layers\n",
    "for p in model.parameters(): p.requires_grad=True\n",
    "opt=AdamW(model.parameters(), lr=BASE_LR/3, weight_decay=WEIGHT_DECAY)\n",
    "sch=CosineAnnealingLR(opt, T_max=FT_EPOCHS)\n",
    "\n",
    "best_val_acc=-1.0; best_state=None\n",
    "for ep in range(1, FT_EPOCHS+1):\n",
    "    tr_loss,tr_acc=train_one_epoch(model, train_dl, opt, device, smoothing=LABEL_SMOOTH)\n",
    "    va_loss,va_acc=evaluate(model, val_dl, device); sch.step()\n",
    "    print(f\"[ft  ][{ep:02d}/{FT_EPOCHS}] train_acc={tr_acc:.4f}  val_acc={va_acc:.4f}\")\n",
    "    if va_acc>best_val_acc:\n",
    "        best_val_acc=va_acc; best_state={k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "best_ckpt=Path(OUTPUT_DIR)/\"vgg16bn_best.pth\"\n",
    "if best_state is not None:\n",
    "    torch.save(best_state, best_ckpt); print(\"Saved best fine-tuned checkpoint:\", best_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd, numpy as np, torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_all(model, dl, device):\n",
    "    model.eval(); ys=[]; ps=[]\n",
    "    for x,y in dl:\n",
    "        x=x.to(device); out=model(x); pred=out.argmax(1).cpu().numpy()\n",
    "        ys.append(y.numpy()); ps.append(pred)\n",
    "    y=np.concatenate(ys); p=np.concatenate(ps); return y,p\n",
    "\n",
    "state=torch.load(best_ckpt, map_location=\"cpu\"); model.load_state_dict(state)\n",
    "test_loss,test_acc=evaluate(model, test_dl, device); y_true,y_pred=predict_all(model,test_dl,device)\n",
    "\n",
    "acc=accuracy_score(y_true,y_pred)\n",
    "prec,rec,f1,_=precision_recall_fscore_support(y_true,y_pred,average=\"macro\",zero_division=0)\n",
    "print(\"Test Accuracy:\", acc); print(\"Macro Precision:\", prec); print(\"Macro Recall:\", rec); print(\"Macro F1:\", f1)\n",
    "\n",
    "df=pd.DataFrame([{\"model\":\"vgg16bn\",\"accuracy\":acc,\"precision_macro\":prec,\"recall_macro\":rec,\"f1_macro\":f1}])\n",
    "csv_path=Path(OUTPUT_DIR)/\"results.csv\"; df.to_csv(csv_path,index=False); csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8,8)); plt.imshow(cm, interpolation='nearest'); plt.title('Confusion Matrix — VGG16-BN (npy splits)')\n",
    "plt.colorbar(); tick_marks=np.arange(len(classes)); plt.xticks(tick_marks, classes, rotation=90); plt.yticks(tick_marks, classes)\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout()\n",
    "cm_path=Path(OUTPUT_DIR)/\"confusion_matrix_vgg16bn_realwaste_splits.png\"\n",
    "plt.savefig(cm_path, dpi=150, bbox_inches=\"tight\"); cm_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64698ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np, seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=classes, yticklabels=classes,\n",
    "            cbar_kws={'label': 'Proportion'}, vmin=0, vmax=1)\n",
    "plt.title('Confusion Matrix — VGG16-BN (npy splits)', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('True', fontsize=11, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = Path(OUTPUT_DIR)/\"confusion_matrix_vgg16bn_realwaste_splits.png\"\n",
    "plt.savefig(cm_path, dpi=150, bbox_inches=\"tight\")\n",
    "cm_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
