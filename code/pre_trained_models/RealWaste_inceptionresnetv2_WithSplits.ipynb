{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d43bd7",
   "metadata": {},
   "source": [
    "# RealWaste â€” Inception-ResNet-v2 (Using Manifest + Numpy Splits)\n",
    "\n",
    "Loads dataset from your `.npy` splits (`filepaths.npy`, `labels_encoded.npy`, `class_names.npy`,\n",
    "`split_train.npy`, `split_val.npy`, `split_test.npy`, optional `mean_std.npy`), fine-tunes\n",
    "**Inception-ResNet-v2**, and reports Accuracy + macro Precision/Recall/F1 + confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Installs (uncomment if needed)\n",
    "# !pip install timm\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install scikit-learn matplotlib pandas tqdm pillow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "SPLITS_DIR = Path(\"./\")  # change if your npy files are elsewhere\n",
    "\n",
    "FILEPATHS_NPY = SPLITS_DIR / \"filepaths.npy\"\n",
    "LABELS_NPY    = SPLITS_DIR / \"labels_encoded.npy\"\n",
    "CLASSES_NPY   = SPLITS_DIR / \"class_names.npy\"\n",
    "TRAIN_NPY     = SPLITS_DIR / \"split_train.npy\"\n",
    "VAL_NPY       = SPLITS_DIR / \"split_val.npy\"\n",
    "TEST_NPY      = SPLITS_DIR / \"split_test.npy\"\n",
    "MEAN_STD_NPY  = SPLITS_DIR / \"mean_std.npy\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"./outputs/inceptionresnetv2_realwaste_splits\").resolve()\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 133\n",
    "BATCH_SIZE = 32\n",
    "HEAD_EPOCHS = 5\n",
    "FT_EPOCHS = 25\n",
    "BASE_LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTH = 0.1\n",
    "IMG_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random, os, numpy as np, torch\n",
    "def set_seed(seed=133):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdc4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load arrays\n",
    "filepaths = np.load(FILEPATHS_NPY, allow_pickle=True)\n",
    "labels    = np.load(LABELS_NPY)\n",
    "classes   = np.load(CLASSES_NPY, allow_pickle=True).tolist()\n",
    "idx_tr    = np.load(TRAIN_NPY); idx_va = np.load(VAL_NPY); idx_te = np.load(TEST_NPY)\n",
    "\n",
    "try:\n",
    "    mean_std = np.load(MEAN_STD_NPY)\n",
    "    mean = mean_std[0].tolist(); std = mean_std[1].tolist()\n",
    "    print(\"Using dataset mean/std:\", mean, std)\n",
    "except Exception:\n",
    "    mean = [0.485, 0.456, 0.406]; std = [0.229, 0.224, 0.225]\n",
    "    print(\"No mean_std.npy found; using ImageNet stats.\")\n",
    "\n",
    "len(classes), classes[:5], len(idx_tr), len(idx_va), len(idx_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class NpySplitDataset(Dataset):\n",
    "    def __init__(self, filepaths, labels, indices, img_size=224, train=True, mean=None, std=None):\n",
    "        self.filepaths=filepaths; self.labels=labels; self.indices=indices; self.train=train\n",
    "        self.mean = mean or [0.485,0.456,0.406]; self.std = std or [0.229,0.224,0.225]\n",
    "        if train:\n",
    "            self.tf = transforms.Compose([\n",
    "                transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
    "                transforms.RandomHorizontalFlip(p=0.5), transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "                transforms.ToTensor(), transforms.Normalize(self.mean,self.std),\n",
    "            ])\n",
    "        else:\n",
    "            self.tf = transforms.Compose([\n",
    "                transforms.Resize(int(img_size*1.15)), transforms.CenterCrop(img_size),\n",
    "                transforms.ToTensor(), transforms.Normalize(self.mean,self.std),\n",
    "            ])\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self,i):\n",
    "        idx=int(self.indices[i]); fp=str(self.filepaths[idx]); y=int(self.labels[idx])\n",
    "        img=Image.open(fp).convert(\"RGB\"); x=self.tf(img); return x,y\n",
    "\n",
    "train_ds=NpySplitDataset(filepaths,labels,idx_tr,IMG_SIZE,True,mean,std)\n",
    "val_ds  =NpySplitDataset(filepaths,labels,idx_va,IMG_SIZE,False,mean,std)\n",
    "test_ds =NpySplitDataset(filepaths,labels,idx_te,IMG_SIZE,False,mean,std)\n",
    "\n",
    "train_dl=DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=4,pin_memory=True)\n",
    "val_dl  =DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=False,num_workers=4,pin_memory=True)\n",
    "test_dl =DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,num_workers=4,pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inception-ResNet-v2 via timm\n",
    "# If timm isn't installed, uncomment:  # !pip install timm\n",
    "import torch.nn as nn\n",
    "try:\n",
    "    import timm\n",
    "except Exception as e:\n",
    "    print(\"timm not installed. Install with: pip install timm\")\n",
    "    raise\n",
    "m = timm.create_model(\"inception_resnet_v2\", pretrained=True, num_classes=len(classes))\n",
    "model = m.to(device)\n",
    "# timm sets the classifier with num_classes; we fine-tune it (and then all layers)\n",
    "classifier_params = [p for p in model.parameters() if p.requires_grad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2977b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dl, optimizer, device, smoothing=0.1):\n",
    "    model.train(); total=correct=0; loss_sum=0.0\n",
    "    for x,y in tqdm(dl, leave=False):\n",
    "        x,y=x.to(device),y.to(device); optimizer.zero_grad()\n",
    "        out=model(x); loss=F.cross_entropy(out,y,label_smoothing=smoothing); pred=out.argmax(1)\n",
    "        loss.backward(); optimizer.step()\n",
    "        bs=y.size(0); loss_sum+=loss.item()*bs; correct+=(pred==y).sum().item(); total+=bs\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl, device):\n",
    "    model.eval(); total=correct=0; loss_sum=0.0\n",
    "    for x,y in dl:\n",
    "        x,y=x.to(device),y.to(device); out=model(x); loss=F.cross_entropy(out,y); pred=out.argmax(1)\n",
    "        bs=y.size(0); loss_sum+=loss.item()*bs; correct+=(pred==y).sum().item(); total+=bs\n",
    "    return loss_sum/total, correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Train classifier head only\n",
    "for p in model.parameters(): p.requires_grad=False\n",
    "for p in classifier_params: p.requires_grad=True\n",
    "\n",
    "opt=AdamW(filter(lambda p:p.requires_grad, model.parameters()), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "sch=CosineAnnealingLR(opt, T_max=HEAD_EPOCHS)\n",
    "\n",
    "best_val_acc=-1.0; best_state=None\n",
    "for ep in range(1, HEAD_EPOCHS+1):\n",
    "    tr_loss,tr_acc=train_one_epoch(model, train_dl, opt, device, smoothing=LABEL_SMOOTH)\n",
    "    va_loss,va_acc=evaluate(model, val_dl, device); sch.step()\n",
    "    print(f\"[head][{ep:02d}/{HEAD_EPOCHS}] train_acc={tr_acc:.4f}  val_acc={va_acc:.4f}\")\n",
    "    if va_acc>best_val_acc:\n",
    "        best_val_acc=va_acc; best_state={k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "from pathlib import Path; import torch\n",
    "if best_state is not None:\n",
    "    head_ckpt=Path(OUTPUT_DIR)/\"inceptionresnetv2_head_best.pth\"\n",
    "    torch.save(best_state, head_ckpt); print(\"Saved:\", head_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Fine-tune all layers\n",
    "for p in model.parameters(): p.requires_grad=True\n",
    "opt=AdamW(model.parameters(), lr=BASE_LR/3, weight_decay=WEIGHT_DECAY)\n",
    "sch=CosineAnnealingLR(opt, T_max=FT_EPOCHS)\n",
    "\n",
    "best_val_acc=-1.0; best_state=None\n",
    "for ep in range(1, FT_EPOCHS+1):\n",
    "    tr_loss,tr_acc=train_one_epoch(model, train_dl, opt, device, smoothing=LABEL_SMOOTH)\n",
    "    va_loss,va_acc=evaluate(model, val_dl, device); sch.step()\n",
    "    print(f\"[ft  ][{ep:02d}/{FT_EPOCHS}] train_acc={tr_acc:.4f}  val_acc={va_acc:.4f}\")\n",
    "    if va_acc>best_val_acc:\n",
    "        best_val_acc=va_acc; best_state={k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "best_ckpt=Path(OUTPUT_DIR)/\"inceptionresnetv2_best.pth\"\n",
    "if best_state is not None:\n",
    "    torch.save(best_state, best_ckpt); print(\"Saved best fine-tuned checkpoint:\", best_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d747cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd, numpy as np, torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_all(model, dl, device):\n",
    "    model.eval(); ys=[]; ps=[]\n",
    "    for x,y in dl:\n",
    "        x=x.to(device); out=model(x); pred=out.argmax(1).cpu().numpy()\n",
    "        ys.append(y.numpy()); ps.append(pred)\n",
    "    y=np.concatenate(ys); p=np.concatenate(ps); return y,p\n",
    "\n",
    "state=torch.load(best_ckpt, map_location=\"cpu\"); model.load_state_dict(state)\n",
    "test_loss,test_acc=evaluate(model, test_dl, device); y_true,y_pred=predict_all(model,test_dl,device)\n",
    "\n",
    "acc=accuracy_score(y_true,y_pred)\n",
    "prec,rec,f1,_=precision_recall_fscore_support(y_true,y_pred,average=\"macro\",zero_division=0)\n",
    "print(\"Test Accuracy:\", acc); print(\"Macro Precision:\", prec); print(\"Macro Recall:\", rec); print(\"Macro F1:\", f1)\n",
    "\n",
    "df=pd.DataFrame([{\"model\":\"inceptionresnetv2\",\"accuracy\":acc,\"precision_macro\":prec,\"recall_macro\":rec,\"f1_macro\":f1}])\n",
    "csv_path=Path(OUTPUT_DIR)/\"results.csv\"; df.to_csv(csv_path,index=False); csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8,8)); plt.imshow(cm, interpolation='nearest'); plt.title('Confusion Matrix â€” Inception-ResNet-v2 (npy splits)')\n",
    "plt.colorbar(); tick_marks=np.arange(len(classes)); plt.xticks(tick_marks, classes, rotation=90); plt.yticks(tick_marks, classes)\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout()\n",
    "cm_path=Path(OUTPUT_DIR)/\"confusion_matrix_inceptionresnetv2_realwaste_splits.png\"\n",
    "plt.savefig(cm_path, dpi=150, bbox_inches=\"tight\"); cm_path\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
