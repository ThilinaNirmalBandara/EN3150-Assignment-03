{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080596da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of classes: 9\n",
      "Classes: [0 1 2 3 4 5 6 7 8]\n",
      "Class distribution: [461 411 420 790 495 500 921 318 436]\n",
      "Train size: 3230, Val size: 571, Test size: 951\n",
      "Using Custom Improved CNN\n",
      "Model parameters: 8,242,185\n",
      "Class weights: tensor([1.1430, 1.2817, 1.2593, 0.6683, 1.0681, 1.0556, 0.5742, 1.6615, 1.2084],\n",
      "       device='cuda:0')\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  6.98it/s, acc=0.14, loss=2.35] \n",
      "Epoch 1/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.22it/s, acc=0.14, loss=2.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.3505 | Train Acc: 0.1399 | Val Loss: 2.1412 | Val Acc: 0.1401\n",
      "✓ Best model saved with val_acc: 0.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|██████████| 202/202 [00:27<00:00,  7.23it/s, acc=0.179, loss=2.12]\n",
      "Epoch 2/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.53it/s, acc=0.163, loss=2.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 2.1235 | Train Acc: 0.1789 | Val Loss: 2.1612 | Val Acc: 0.1629\n",
      "✓ Best model saved with val_acc: 0.1629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  7.18it/s, acc=0.195, loss=2.09]\n",
      "Epoch 3/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.93it/s, acc=0.156, loss=2.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 2.0881 | Train Acc: 0.1947 | Val Loss: 2.1048 | Val Acc: 0.1559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  6.99it/s, acc=0.195, loss=2.06]\n",
      "Epoch 4/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.28it/s, acc=0.107, loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 2.0606 | Train Acc: 0.1954 | Val Loss: 2.3397 | Val Acc: 0.1068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|██████████| 202/202 [00:29<00:00,  6.95it/s, acc=0.211, loss=2.05]\n",
      "Epoch 5/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 13.51it/s, acc=0.235, loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 2.0492 | Train Acc: 0.2105 | Val Loss: 1.9845 | Val Acc: 0.2347\n",
      "✓ Best model saved with val_acc: 0.2347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|██████████| 202/202 [00:29<00:00,  6.91it/s, acc=0.229, loss=2]   \n",
      "Epoch 6/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 15.19it/s, acc=0.294, loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 2.0033 | Train Acc: 0.2294 | Val Loss: 1.8666 | Val Acc: 0.2942\n",
      "✓ Best model saved with val_acc: 0.2942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  7.08it/s, acc=0.252, loss=1.97]\n",
      "Epoch 7/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.46it/s, acc=0.247, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 1.9684 | Train Acc: 0.2517 | Val Loss: 2.1249 | Val Acc: 0.2469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  7.16it/s, acc=0.272, loss=1.93]\n",
      "Epoch 8/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 13.99it/s, acc=0.215, loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 1.9348 | Train Acc: 0.2724 | Val Loss: 2.1450 | Val Acc: 0.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  7.14it/s, acc=0.266, loss=1.94]\n",
      "Epoch 9/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.78it/s, acc=0.203, loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 1.9425 | Train Acc: 0.2659 | Val Loss: 2.2139 | Val Acc: 0.2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Train]: 100%|██████████| 202/202 [00:27<00:00,  7.34it/s, acc=0.288, loss=1.9] \n",
      "Epoch 10/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.38it/s, acc=0.205, loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 1.9048 | Train Acc: 0.2879 | Val Loss: 2.1851 | Val Acc: 0.2049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Train]: 100%|██████████| 202/202 [00:27<00:00,  7.23it/s, acc=0.307, loss=1.83]\n",
      "Epoch 11/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.73it/s, acc=0.32, loss=1.79] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train Loss: 1.8308 | Train Acc: 0.3065 | Val Loss: 1.7913 | Val Acc: 0.3205\n",
      "✓ Best model saved with val_acc: 0.3205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Train]: 100%|██████████| 202/202 [00:27<00:00,  7.27it/s, acc=0.32, loss=1.82] \n",
      "Epoch 12/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 15.59it/s, acc=0.35, loss=1.71] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train Loss: 1.8171 | Train Acc: 0.3204 | Val Loss: 1.7105 | Val Acc: 0.3503\n",
      "✓ Best model saved with val_acc: 0.3503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Train]: 100%|██████████| 202/202 [00:27<00:00,  7.23it/s, acc=0.334, loss=1.81]\n",
      "Epoch 13/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.79it/s, acc=0.368, loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train Loss: 1.8079 | Train Acc: 0.3344 | Val Loss: 1.6824 | Val Acc: 0.3678\n",
      "✓ Best model saved with val_acc: 0.3678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  7.16it/s, acc=0.354, loss=1.8] \n",
      "Epoch 14/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 15.79it/s, acc=0.373, loss=1.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train Loss: 1.7998 | Train Acc: 0.3539 | Val Loss: 1.6870 | Val Acc: 0.3730\n",
      "✓ Best model saved with val_acc: 0.3730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 [Train]: 100%|██████████| 202/202 [00:28<00:00,  7.15it/s, acc=0.372, loss=1.77]\n",
      "Epoch 15/50 [Val]: 100%|██████████| 36/36 [00:02<00:00, 16.22it/s, acc=0.401, loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Train Loss: 1.7665 | Train Acc: 0.3721 | Val Loss: 1.6248 | Val Acc: 0.4011\n",
      "✓ Best model saved with val_acc: 0.4011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 [Train]:  50%|█████     | 102/202 [00:14<00:13,  7.37it/s, acc=0.353, loss=1.78]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------\n",
    "# 0. Device\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load data\n",
    "# ---------------------------\n",
    "image_array = np.load(\"images.npy\", allow_pickle=True)\n",
    "labels = np.load(\"labels.npy\", allow_pickle=True)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(labels)\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {encoder.classes_}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Train/Val/Test split\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(y)), test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_idx, test_size=0.15, random_state=42, stratify=y[train_idx]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_idx)}, Val size: {len(val_idx)}, Test size: {len(test_idx)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Dataset class\n",
    "# ---------------------------\n",
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, images, labels, indices, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        img = self.images[i].astype(np.uint8)\n",
    "        label = self.labels[i]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Enhanced Transforms\n",
    "# ---------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = WasteDataset(image_array, y, train_idx, transform=train_transform)\n",
    "val_dataset = WasteDataset(image_array, y, val_idx, transform=val_transform)\n",
    "test_dataset = WasteDataset(image_array, y, test_idx, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=0, pin_memory=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Custom CNN Model\n",
    "# ---------------------------\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        self.fc1 = nn.Linear(512*4*4, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.adapt_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(\"Using Custom Improved CNN\")\n",
    "model = ImprovedCNN(num_classes, input_channels=3)\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Label Smoothing Loss\n",
    "# ---------------------------\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon=0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        n_class = output.size(1)\n",
    "        log_preds = F.log_softmax(output, dim=1)\n",
    "        loss = -log_preds.sum(dim=1).mean()\n",
    "        nll = F.nll_loss(log_preds, target, weight=self.weight)\n",
    "        return (1 - self.epsilon) * nll + self.epsilon * loss / n_class\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y[train_idx]),\n",
    "    y=y[train_idx]\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "criterion = LabelSmoothingCrossEntropy(epsilon=0.1, weight=class_weights)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Optimizer and Scheduler\n",
    "# ---------------------------\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Training loop\n",
    "# ---------------------------\n",
    "num_epochs = 50\n",
    "best_val_acc = 0\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    \n",
    "    for i, (X_batch, y_batch) in loop:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        loop.set_postfix(loss=running_loss/total, acc=correct/total)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    loop_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loop_val:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            loop_val.set_postfix(loss=val_loss/total, acc=correct/total)\n",
    "\n",
    "    val_loss = val_loss / total\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        trigger_times = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, \"best_model.pth\")\n",
    "        print(f\"✓ Best model saved with val_acc: {val_acc:.4f}\")\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Plot training history\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Test evaluation\n",
    "# ---------------------------\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop_test = tqdm(test_loader, desc=\"Testing\")\n",
    "    for X_batch, y_batch in loop_test:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# 10. Confusion matrix\n",
    "# ---------------------------\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
    "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.ylabel(\"True\", fontsize=12)\n",
    "plt.title(f\"Confusion Matrix (Test Accuracy: {test_acc*100:.2f}%)\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 11. Classification report\n",
    "# ---------------------------\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in encoder.classes_], digits=4))\n",
    "\n",
    "# ---------------------------\n",
    "# 12. Per-class accuracy\n",
    "# ---------------------------\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\"*70)\n",
    "for i, class_name in enumerate(encoder.classes_):\n",
    "    class_mask = np.array(y_true) == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = np.mean(np.array(y_pred)[class_mask] == i)\n",
    "        print(f\"{str(class_name):20s}: {class_acc*100:6.2f}% ({class_mask.sum()} samples)\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"Final test accuracy: {test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
